import os
import time

import streamlit as st
from dotenv import load_dotenv

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 0) Set Streamlit page configuration (MUST be first Streamlit command)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
st.set_page_config(
    page_title="Phama Punjabi Chat (FAISS Load)",
    layout="centered"
)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 1) Load environment variables (for Mistral API key)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
load_dotenv()  # looks for a .env file in this folder or parent

MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY", None)
if not MISTRAL_API_KEY:
    st.warning(
        "โ๏ธ MISTRAL_API_KEY not found in .env. "
        "Make sure to set it if required by LangChain-Mistral."
    )

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 2) Load a pre-built FAISS index from disk (cached to run only once)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
@st.cache_resource(show_spinner="Loading FAISS index from diskโฆ")
def load_vectorstore():
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.vectorstores import FAISS

    # 2.1) Same embedding model used during indexing, force CUDA if available
    EMBED_MODEL = "l3cube-pune/punjabi-sentence-similarity-sbert"
    model_kwargs = {"device": "cuda"} if os.getenv("CUDA_VISIBLE_DEVICES", None) or True else {}
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL, model_kwargs=model_kwargs)

    # 2.2) Path to the saved FAISS index folder
    INDEX_DIR = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..", "faiss_index", "phama_faiss")
    )

    if not os.path.isdir(INDEX_DIR):
        st.error(
            "FAISS index not found on disk! Please run `python build_faiss_index.py` first "
            "to generate the index in 'faiss_index/phama_faiss/'."
        )
        return None

    # 2.3) Load the persisted FAISS index, allowing pickle deserialization
    vectorstore = FAISS.load_local(
        INDEX_DIR, embeddings, allow_dangerous_deserialization=True
    )
    return vectorstore

vectorstore = load_vectorstore()
if vectorstore is None:
    st.stop()

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 3) Initialize the ChatMistralAI LLM and prompt template (cached once)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
@st.cache_resource(show_spinner="Loading prompt template & LLMโฆ")
def init_llm_and_prompt():
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_mistralai import ChatMistralAI

    # 3.1) Punjabi โsystem + humanโ prompt
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """เจคเฉเจธเฉเจ เจเจพเจฒ เจธเฉเจเจเจฐ เจตเจฟเฉฑเจ เจเฉฑเจ เจเจฐเจฎเจเจพเจฐเฉ เจนเฉเฅค
                เจเจชเจฃเฉ เจเฉเจฆ เจฆเฉ เจเจฟเจเจจ เจเจคเฉ เจนเฉเจเจพเจ เจฆเจฟเฉฑเจคเฉ เจคเจฟเฉฐเจจ เจธเฉฐเจฆเจฐเจญเจพเจ เจฆเฉ เจตเจฐเจคเฉเจ เจเจฐเจเฉ
                เจธเจตเจพเจฒ เจฆเจพ เจเฉฑเจ เจนเฉ เจธเฉฐเจฏเฉเจเจค เจเจตเจพเจฌ เจชเฉเจฐเจฆเจพเจจ เจเจฐเฉเฅค
                เจธเฉฐเจฆเจฐเจญ:
                {context}
                """,
            ),
            ("human", "{question}"),
        ]
    )

    # 3.2) Use a valid Mistral model name (e.g., "mistral-large-latest")
    llm = ChatMistralAI(
        model="mistral-large-latest",
        temperature=0
    )

    return prompt, llm

prompt_obj, llm_obj = init_llm_and_prompt()

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 4) Helper: โembed โ retrieve โ LLM โ answerโ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def get_answer(question: str) -> str:
    from langchain_core.runnables import RunnableMap
    from langchain_core.output_parsers import StrOutputParser

    # 4.1) Retrieve top-3 chunks from FAISS
    docs = vectorstore.similarity_search(question, k=3)

    # 4.2) Format them as:
    #     โเจธเฉฐเจฆเจฐเจญ 1: <chunk_text>\n\n
    #      เจธเฉฐเจฆเจฐเจญ 2: <chunk_text>\n\n
    #      เจธเฉฐเจฆเจฐเจญ 3: <chunk_text>โ
    formatted_contexts = "\n\n".join(
        [f"เจธเฉฐเจฆเจฐเจญ {i+1} ({d.metadata['source']}): {d.page_content}" for i, d in enumerate(docs)]
    )

    # 4.3) Build a RunnableMap to feed โcontextโ & โquestionโ into prompt โ LLM
    rag_chain = (
        RunnableMap(
            {
                "context": lambda _: formatted_contexts,
                "question": lambda _: question,
            }
        )
        | prompt_obj
        | llm_obj
        | StrOutputParser()
    )

    # 4.4) Invoke synchronously and return the answer
    answer = rag_chain.invoke({})
    return answer

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# 5) Streamlit UI layout
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
st.title("Punjabi AgroBot")
st.write(
    """
    เจเจชเจฃเจพ เจชเฉฐเจเจพเจฌเฉ เจธเจตเจพเจฒ เจฆเจฟเจ, เจเจคเฉ เจเจน เจเจช เจคเจฟเฉฐเจจเจพเจ เจธเจญ เจคเฉเจ เจขเฉเจเจตเฉเจ เจเฉฐเจเจพเจ เจจเฉเฉฐ เจฒเฉฑเจญ เจเฉ 
    เจเฉฑเจ เจธเฉฐเจฏเฉเจเจค เจเจตเจพเจฌ เจตเจพเจชเจธ เจเจฐเฉเจเจพ (`Mistral-7B` เจตเจฐเจค เจเฉ)เฅค
    """
)

# 5.1) Text area for user to type a Punjabi question
user_question = st.text_area(
    "เจเจชเจฃเจพ เจชเฉฐเจเจพเจฌเฉ เจธเจตเจพเจฒ เจเฉฑเจฅเฉ เจฒเจฟเจเฉ:",
    height=150,
    placeholder="ไพ: เจฎเฉเจ เจฌเฉเจ เจฌเจพเจฐเฉ เจเจพเจฃเจเจพเจฐเฉ เจเจพเจนเฉเฉฐเจฆเจพ เจนเจพเจโฆ",
)

# 5.2) โGet Answerโ button
if st.button("เจเจตเจพเจฌ เจชเฉเจฐเจพเจชเจค เจเจฐเฉ") and user_question.strip():
    with st.spinner("เจธเฉเจ เจฐเจนเฉ เจนเจพเจโฆ"):
        start_time = time.time()
        try:
            answer = get_answer(user_question)
        except Exception as e:
            answer = f"๐จ เจเฉเจ เจเจฒเจคเฉ เจเจ: {e}"
        elapsed = time.time() - start_time

    # 5.3) Show the final answer
    st.subheader("เจฎเจพเจกเจฒ เจฆเจพ เจเจตเจพเจฌ:")
    st.write(answer)
    st.caption(f"Response time: {elapsed:.2f}s")

# 5.4) Sidebar info
with st.sidebar:
    st.header("เจเจน เจเฉ เจนเฉ?")
    st.write(
        "เจเจน Streamlit เจเจช เจชเจนเจฟเจฒเจพเจ เจฌเจฃเจพเจ เจเจ FAISS เจเฉฐเจกเฉเจเจธ เจจเฉเฉฐ เจฒเฉเจก เจเจฐเจฆเจพ เจนเฉเฅค\n\n"
        "1. **เจเฉฐเจกเฉเจเจธ** เจซเจพเจเจฒ เจคเฉเจ เจคเฉเจฐเฉฐเจค FAISS เจตเฉเจเจเจฐเจธเจเฉเจฐ เจฒเฉเจก เจนเฉ เจเจพเจเจฆเจพ เจนเฉเฅค\n"
        "2. **Punjabi SBERT** เจตเจฐเจค เจเฉ เจคเฉเจนเจพเจกเฉ เจธเจตเจพเจฒ เจจเฉเฉฐ embed เจเจฐเจฆเจพ เจนเฉ (CUDA เจธเจฎเจฐเจฅเจจ)เฅค\n"
        "3. FAISS เจตเจฟเฉฑเจเฉเจ เจเจพเจช-3 เจเฉฐเจ เจฒเจฟเจเจเจเจฆเจพ เจนเฉเฅค\n"
        "4. เจเจน เจเฉฐเจ **เจเจคเฉ** เจคเฉเจนเจพเจกเจพ เจธเจตเจพเจฒ `Mistral-7B` เจจเฉเฉฐ เจญเฉเจ เจเฉ เจเจตเจพเจฌ เจฒเฉเจเจฆเจพ เจนเฉเฅค\n\n"
        "เจเจฆเฉ เจตเฉ FAISS เจเฉฐเจกเฉเจเจธ เจจเจนเฉเจ เจฌเจฃเจพเจเจเจฆเจพโเจธเจฟเจฐเจซเจผ `build_faiss_index.py` เจเจฒเจพเจเจฃ เจจเจพเจฒ เจชเจนเจฟเจฒเจพเจ เจนเฉ เจเฉฐเจกเฉเจเจธ เจฌเจฃ เจเฉเฉฑเจเจพ เจนเฉเฉฐเจฆเจพ เจนเฉเฅค"
    )
    st.markdown("[GitHub: Punjabi Agrobot](https://github.com/WakeUpSidd/Punjabi-AgroBot)")
